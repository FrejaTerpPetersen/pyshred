Fri Apr 25 15:29:14 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:3A:00.0 Off |                    0 |
| N/A   36C    P0             69W /  300W |       0MiB /  32768MiB |      0%   E. Process |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Loaded packages ... Let's start working...

num_sensors changed to  3 to match sensor location file
Using cuda:  True 

Training epoch 1
Error tensor(0.1223, device='cuda:0')
Training epoch 20
Error tensor(0.0313, device='cuda:0')
Training epoch 40
Error tensor(0.0194, device='cuda:0')
Training epoch 60
Error tensor(0.0124, device='cuda:0')
Training epoch 80
Error tensor(0.0123, device='cuda:0')
Training epoch 100
Error tensor(0.0086, device='cuda:0')
Training epoch 120
Error tensor(0.0117, device='cuda:0')
Training epoch 140
Error tensor(0.0071, device='cuda:0')
Training epoch 160
Error tensor(0.0100, device='cuda:0')
Training epoch 180
Error tensor(0.0058, device='cuda:0')
Training epoch 200
Error tensor(0.0064, device='cuda:0')
Training epoch 220
Error tensor(0.0047, device='cuda:0')
Training epoch 240
Error tensor(0.0114, device='cuda:0')
Training epoch 260
Error tensor(0.0052, device='cuda:0')
Training epoch 280
Error tensor(0.0058, device='cuda:0')
Training epoch 300
Error tensor(0.0088, device='cuda:0')
Training epoch 320
Error tensor(0.0068, device='cuda:0')
Training epoch 1
Error tensor(0.1672, device='cuda:0')
Training epoch 20
Error tensor(0.0171, device='cuda:0')
Training epoch 40
Error tensor(0.0137, device='cuda:0')
Training epoch 60
Error tensor(0.0121, device='cuda:0')
Training epoch 80
Error tensor(0.0077, device='cuda:0')
Training epoch 100
Error tensor(0.0112, device='cuda:0')
Training epoch 120
Error tensor(0.0096, device='cuda:0')
Training epoch 140
Error tensor(0.0145, device='cuda:0')
Training epoch 160
Error tensor(0.0071, device='cuda:0')
Training epoch 180
Error tensor(0.0078, device='cuda:0')
Training epoch 200
Error tensor(0.0077, device='cuda:0')
Training epoch 220
Error tensor(0.0066, device='cuda:0')
Training epoch 240
Error tensor(0.0065, device='cuda:0')
Training epoch 260
Error tensor(0.0156, device='cuda:0')
Training epoch 280
Error tensor(0.0054, device='cuda:0')
Training epoch 300
Error tensor(0.0069, device='cuda:0')
Training epoch 320
Error tensor(0.0120, device='cuda:0')
Training epoch 340
Error tensor(0.0115, device='cuda:0')
Training epoch 360
Error tensor(0.0058, device='cuda:0')
Training epoch 380
Error tensor(0.0078, device='cuda:0')

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 24802151: <forecasts> in cluster <dcc> Done

Job <forecasts> was submitted from host <n-62-30-1> by user <ftmp> in cluster <dcc> at Fri Apr 25 15:26:50 2025
Job was executed on host(s) <4*n-62-20-11>, in queue <gpuv100>, as user <ftmp> in cluster <dcc> at Fri Apr 25 15:29:12 2025
</zhome/cf/9/138047> was used as the home directory.
</zhome/cf/9/138047/pyshred> was used as the working directory.
Started at Fri Apr 25 15:29:12 2025
Terminated at Fri Apr 25 15:32:27 2025
Results reported at Fri Apr 25 15:32:27 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh 
### General options 
### -- specify queue -- 
#BSUB -q gpuv100
### -- set the job Name -- 
#BSUB -J forecasts
### -- ask for number of cores (default: 1) -- 
#BSUB -n 4
### -- Select the resources: 1 gpu in exclusive process mode --
#BSUB -gpu "num=1:mode=exclusive_process"
### -- specify that the cores must be on the same host -- 
#BSUB -R "span[hosts=1]"
### -- specify that we need 4GB of memory per core/slot -- 
#BSUB -R "rusage[mem=2GB]"
### -- specify that we want the job to get killed if it exceeds 5 GB per core/slot -- 
#BSUB -M 3GB
### -- set walltime limit: hh:mm -- 
#BSUB -W 24:00 
### -- Specify the output and error file. %J is the job-id -- 
### -- -o and -e mean append, -oo and -eo mean overwrite -- 
#BSUB -o outputs_hpc/Output_%J.out 
#BSUB -e outputs_hpc/Output_%J.err 


nvidia-smi
# Load the cuda module
module load cuda/11.6

source ../envs/envs/shred/bin/activate

# here follow the commands you want to execute with input.in as the input file
python -u forecasts.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   189.00 sec.
    Max Memory :                                 1131 MB
    Average Memory :                             1028.50 MB
    Total Requested Memory :                     8192.00 MB
    Delta Memory :                               7061.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                11
    Run time :                                   197 sec.
    Turnaround time :                            337 sec.

The output (if any) is above this job summary.



PS:

Read file <outputs_hpc/Output_24802151.err> for stderr output of this job.

