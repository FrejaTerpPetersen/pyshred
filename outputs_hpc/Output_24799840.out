Loaded packages ... Let's start working...

Using cuda:  False
Training epoch 1
Error tensor(0.1697)
Training epoch 20
Error tensor(0.0230)
Training epoch 40
Error tensor(0.0100)
Training epoch 60
Error tensor(0.0166)
Training epoch 80
Error tensor(0.0155)
Training epoch 100
Error tensor(0.0110)
Training epoch 120
Error tensor(0.0105)
Training epoch 140
Error tensor(0.0096)
Training epoch 160
Error tensor(0.0139)
Training epoch 180
Error tensor(0.0092)
Training epoch 200
Error tensor(0.0155)
Training epoch 220
Error tensor(0.0094)
Training epoch 240
Error tensor(0.0076)
Training epoch 260
Error tensor(0.0087)
Training epoch 280
Error tensor(0.0083)
Training epoch 300
Error tensor(0.0063)
Training epoch 320
Error tensor(0.0063)
Training epoch 340
Error tensor(0.0050)
Training epoch 360
Error tensor(0.0172)
Training epoch 380
Error tensor(0.0092)
Training epoch 400
Error tensor(0.0077)
Training epoch 420
Error tensor(0.0055)
Training epoch 440
Error tensor(0.0073)
Training epoch 1
Error tensor(0.1590)
Training epoch 20
Error tensor(0.0225)
Training epoch 40
Error tensor(0.0166)
Training epoch 60
Error tensor(0.0115)
Training epoch 80
Error tensor(0.0119)
Training epoch 100
Error tensor(0.0132)
Training epoch 120
Error tensor(0.0076)
Training epoch 140
Error tensor(0.0109)
Training epoch 160
Error tensor(0.0075)
Training epoch 180
Error tensor(0.0133)
Training epoch 200
Error tensor(0.0061)
Training epoch 220
Error tensor(0.0079)
Training epoch 240
Error tensor(0.0083)
Training epoch 260
Error tensor(0.0062)
Training epoch 280
Error tensor(0.0105)
Training epoch 300
Error tensor(0.0102)

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 24799840: <forecasts> in cluster <dcc> Done

Job <forecasts> was submitted from host <n-62-27-20> by user <ftmp> in cluster <dcc> at Fri Apr 25 11:26:32 2025
Job was executed on host(s) <n-62-31-3>, in queue <hpc>, as user <ftmp> in cluster <dcc> at Fri Apr 25 11:26:32 2025
</zhome/cf/9/138047> was used as the home directory.
</zhome/cf/9/138047/pyshred> was used as the working directory.
Started at Fri Apr 25 11:26:32 2025
Terminated at Fri Apr 25 11:57:40 2025
Results reported at Fri Apr 25 11:57:40 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh 
### General options 
### -- specify queue -- 
#BSUB -q hpc
### -- set the job Name -- 
#BSUB -J forecasts
### -- ask for number of cores (default: 1) -- 
#BSUB -n 1
### -- specify that the cores must be on the same host -- 
#BSUB -R "span[hosts=1]"
### -- specify that we need 4GB of memory per core/slot -- 
#BSUB -R "rusage[mem=10GB]"
### -- specify that we want the job to get killed if it exceeds 5 GB per core/slot -- 
#BSUB -M 10GB
### -- set walltime limit: hh:mm -- 
#BSUB -W 24:00 
### -- Specify the output and error file. %J is the job-id -- 
### -- -o and -e mean append, -oo and -eo mean overwrite -- 
#BSUB -o outputs_hpc/Output_%J.out 
#BSUB -e outputs_hpc/Output_%J.err 

source ../envs/envs/shred/bin/activate

# here follow the commands you want to execute with input.in as the input file
python -u forecasts.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1865.59 sec.
    Max Memory :                                 805 MB
    Average Memory :                             759.14 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               9435.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                6
    Run time :                                   1945 sec.
    Turnaround time :                            1868 sec.

The output (if any) is above this job summary.



PS:

Read file <outputs_hpc/Output_24799840.err> for stderr output of this job.

