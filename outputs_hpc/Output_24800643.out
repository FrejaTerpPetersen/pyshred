Fri Apr 25 13:57:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:3A:00.0 Off |                    0 |
| N/A   36C    P0             69W /  300W |       0MiB /  32768MiB |      0%   E. Process |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Loaded packages ... Let's start working...

Using cuda:  True 

Training epoch 1
Error tensor(0.0991, device='cuda:0')
Training epoch 20
Error tensor(0.0269, device='cuda:0')
Training epoch 40
Error tensor(0.0171, device='cuda:0')
Training epoch 60
Error tensor(0.0120, device='cuda:0')
Training epoch 80
Error tensor(0.0126, device='cuda:0')
Training epoch 100
Error tensor(0.0173, device='cuda:0')
Training epoch 120
Error tensor(0.0109, device='cuda:0')
Training epoch 140
Error tensor(0.0081, device='cuda:0')
Training epoch 160
Error tensor(0.0094, device='cuda:0')
Training epoch 180
Error tensor(0.0081, device='cuda:0')
Training epoch 200
Error tensor(0.0099, device='cuda:0')
Training epoch 220
Error tensor(0.0095, device='cuda:0')
Training epoch 240
Error tensor(0.0086, device='cuda:0')
Training epoch 260
Error tensor(0.0084, device='cuda:0')
Training epoch 280
Error tensor(0.0106, device='cuda:0')
Training epoch 1
Error tensor(0.1250, device='cuda:0')
Training epoch 20
Error tensor(0.0118, device='cuda:0')
Training epoch 40
Error tensor(0.0172, device='cuda:0')
Training epoch 60
Error tensor(0.0124, device='cuda:0')
Training epoch 80
Error tensor(0.0069, device='cuda:0')
Training epoch 100
Error tensor(0.0056, device='cuda:0')
Training epoch 120
Error tensor(0.0180, device='cuda:0')
Training epoch 140
Error tensor(0.0174, device='cuda:0')
Training epoch 160
Error tensor(0.0061, device='cuda:0')
Training epoch 180
Error tensor(0.0089, device='cuda:0')
Training epoch 200
Error tensor(0.0125, device='cuda:0')

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 24800643: <forecasts> in cluster <dcc> Done

Job <forecasts> was submitted from host <n-62-30-7> by user <ftmp> in cluster <dcc> at Fri Apr 25 13:57:50 2025
Job was executed on host(s) <4*n-62-20-11>, in queue <gpuv100>, as user <ftmp> in cluster <dcc> at Fri Apr 25 13:57:56 2025
</zhome/cf/9/138047> was used as the home directory.
</zhome/cf/9/138047/pyshred> was used as the working directory.
Started at Fri Apr 25 13:57:56 2025
Terminated at Fri Apr 25 14:00:36 2025
Results reported at Fri Apr 25 14:00:36 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh 
### General options 
### -- specify queue -- 
#BSUB -q gpuv100
### -- set the job Name -- 
#BSUB -J forecasts
### -- ask for number of cores (default: 1) -- 
#BSUB -n 4
### -- Select the resources: 1 gpu in exclusive process mode --
#BSUB -gpu "num=1:mode=exclusive_process"
### -- specify that the cores must be on the same host -- 
#BSUB -R "span[hosts=1]"
### -- specify that we need 4GB of memory per core/slot -- 
#BSUB -R "rusage[mem=5GB]"
### -- specify that we want the job to get killed if it exceeds 5 GB per core/slot -- 
#BSUB -M 5GB
### -- set walltime limit: hh:mm -- 
#BSUB -W 24:00 
### -- Specify the output and error file. %J is the job-id -- 
### -- -o and -e mean append, -oo and -eo mean overwrite -- 
#BSUB -o outputs_hpc/Output_%J.out 
#BSUB -e outputs_hpc/Output_%J.err 


nvidia-smi
# Load the cuda module
module load cuda/11.6

source ../envs/envs/shred/bin/activate

# here follow the commands you want to execute with input.in as the input file
python -u forecasts.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   146.00 sec.
    Max Memory :                                 1140 MB
    Average Memory :                             1037.25 MB
    Total Requested Memory :                     20480.00 MB
    Delta Memory :                               19340.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                11
    Run time :                                   160 sec.
    Turnaround time :                            166 sec.

The output (if any) is above this job summary.



PS:

Read file <outputs_hpc/Output_24800643.err> for stderr output of this job.

